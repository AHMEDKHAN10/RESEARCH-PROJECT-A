{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymoo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d6f44ac009de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepair\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRepair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProblem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymoo'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Optimisation\n",
    "from pymoo.model.repair import Repair\n",
    "from pymoo.model.problem import Problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.algorithms.nsga2 import NSGA2\n",
    "from pymoo.factory import *\n",
    "from pymoo.model.sampling import Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the results of each of the used classifier as input instade of inputting only one classifier result\n",
    "df1 = pd.read_csv(\"nn.csv\")\n",
    "df2 = pd.read_csv(\"nb.csv\")\n",
    "dfs = [df1 , df2]\n",
    "inputs = {\n",
    "    \"id_column\": \"id\",\n",
    "    \"ground_truth_column\": \"ground_truth\",\n",
    "    \"reject_label\": \"REJECT\",\n",
    "    \"min\": 0,\n",
    "    \"max\": 1,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matches(all_probabilities, lower, upper):\n",
    "    not_match = np.less(all_probabilities[0], lower)\n",
    "    match = np.greater_equal(all_probabilities[0], upper)\n",
    "    for prob in all_probabilities[1:]:\n",
    "        not_match_temp = np.less(prob, lower)\n",
    "        not_match = np.logical_or(not_match, not_match_temp)\n",
    "        \n",
    "        match_temp = np.greater_equal(prob, upper)\n",
    "        match = np.logical_or(match, match_temp)\n",
    "    \n",
    "    rejects = ~np.logical_xor(not_match, match)\n",
    "    return np.stack([not_match, match, rejects])\n",
    "\n",
    "def apply_thresholds(all_probabilities, lower, upper):\n",
    "    temp = calculate_matches(all_probabilities, lower, upper)\n",
    "    def value(x):\n",
    "        return np.where(x == True)[0][0]\n",
    "    return np.apply_along_axis(value, 0, temp)\n",
    " \n",
    "def calculate_confusion_matrices(gt, pred):\n",
    "    result = np.zeros((gt.shape[1],3,3), dtype=np.int)\n",
    "    for i,(x,y) in enumerate(zip(gt.T,pred.T)):\n",
    "        matrix = confusion_matrix(x,y)\n",
    "        if np.shape(matrix) < (3,3):\n",
    "            matrix = np.c_[matrix, np.zeros(2)]\n",
    "            matrix = np.r_[matrix, [np.zeros(3)]]\n",
    "        elif np.shape(matrix) > (3, 3):\n",
    "            raise ValueError(\"Matrix should be 3x3, error in input labels\")\n",
    "\n",
    "        # Column order: matches, not match, rejects\n",
    "        matrix[:,[0,1]] = matrix[:,[1, 0]]\n",
    "        # Row order: matches, not match, rejects \n",
    "        matrix[[0,1], :] = matrix[[1, 0], :]\n",
    "\n",
    "        result[i] = matrix\n",
    "    return result\n",
    "\n",
    "def normalise_probs_in_place(df, inputs, labels):\n",
    "    if inputs[\"min\"] == 0 and inputs[\"max\"] == 100:\n",
    "        for label in labels:\n",
    "            if label == inputs[\"reject_label\"]:\n",
    "                continue\n",
    "            if (df[label] < 1).any() and (df[label] >= 0).any():\n",
    "                return \n",
    "            df[label] = df[label] / 100      \n",
    "    elif inputs[\"min\"] == 0 and inputs[\"max\"] == 1:\n",
    "        return\n",
    "    else:\n",
    "        raise ValueError(\"Normalisation rule not specified\")\n",
    "\n",
    "def derive_probabilities(dfs, inputs):\n",
    "    if \"probability_column\" in inputs:\n",
    "        ##NA##\n",
    "        df = dfs[0]\n",
    "        probabilities = pd.DataFrame()\n",
    "        probabilities[inputs[\"target_label\"]] = df[inputs[\"probability_column\"]]\n",
    "        probabilities[\"id\"] = df[inputs[\"id_column\"]]\n",
    "    else:\n",
    "        labels = list(retrieve_labels(dfs[0], inputs))\n",
    "        labels.insert(0, inputs[\"id_column\"])\n",
    "        all_probabilities = []\n",
    "        for df in dfs:\n",
    "            probabilities = df[labels]\n",
    "            probabilities = probabilities.rename(columns={inputs[\"id_column\"]: \"id\"})\n",
    "            probabilities.drop_duplicates(\"id\")\n",
    "            all_probabilities.append(probabilities)\n",
    "    return all_probabilities\n",
    "\n",
    "def retrieve_labels(df, inputs):\n",
    "    return df[inputs[\"ground_truth_column\"]].str.strip().sort_values().unique()\n",
    "\n",
    "def check_labels_have_columns(df, labels):\n",
    "    return len(set(df.columns) & set(labels)) == len(labels)\n",
    "\n",
    "def prepare_labels(df, inputs):\n",
    "    labels = retrieve_labels(df[0], inputs)\n",
    "\n",
    "    if \"target_label\" in inputs:\n",
    "        labels = list(filter(lambda x: x == inputs[\"target_label\"], labels))\n",
    "    \n",
    "    if not check_labels_have_columns(df[0], labels) and not \"probability_column\" in inputs:\n",
    "        raise ValueError(\"Labels do not have column names for probabilities\")   \n",
    "        \n",
    "    return sorted(labels)\n",
    "\n",
    "\n",
    "def prepare_ground_truth(df, inputs, mapping):\n",
    "    ids = []\n",
    "    truth_columns = inputs[\"ground_truth_column\"]\n",
    "\n",
    "    if not \"target_label\" in inputs:\n",
    "        all_labels = []\n",
    "        for a_id in df[inputs[\"id_column\"]].unique():\n",
    "            ground_truth_labels = df[df[inputs[\"id_column\"]] == a_id][truth_columns]\n",
    "            indexes = [mapping[s.strip()] for s in list(ground_truth_labels)]     \n",
    "            ground_truth = np.zeros(len(mapping), dtype=int)\n",
    "            ground_truth[indexes] = 1\n",
    "            ids.append(a_id)\n",
    "            all_labels.append(ground_truth)\n",
    "        columns = list(mapping.keys())\n",
    "        ground_truth = pd.DataFrame(all_labels)\n",
    "        ground_truth.columns = columns\n",
    "    else:\n",
    "        ids = df[inputs[\"id_column\"]].unique()\n",
    "        ground_truth = pd.DataFrame()\n",
    "        ground_truth[inputs[\"target_label\"]] = (df[truth_columns] == inputs[\"target_label\"]) * 1\n",
    "    ground_truth[\"id\"] = ids    \n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def get_objective(all_probabilities, ground_truth, lower_thresholds, upper_thresholds):\n",
    "    \"\"\"\n",
    "    returns: (all confusion matrices, thresholds) \n",
    "    \"\"\"\n",
    "    thres = apply_thresholds(all_probabilities, lower_thresholds, upper_thresholds)  \n",
    "    all_results = calculate_confusion_matrices(ground_truth, thres)\n",
    "    return all_results, thres\n",
    "\n",
    "def summarise_results(all_results):\n",
    "    \"\"\"\n",
    "    returns Summary array of true match, \n",
    "    false match, missed match, and rejects, \n",
    "    \"\"\"\n",
    "    return np.array([all_results[:,0][:,0].sum(),\n",
    "            all_results[:,1][:,0].sum(),\n",
    "            all_results[:,0][:,1].sum(),\n",
    "            all_results[:,:,2].sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage guide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = prepare_labels(dfs, inputs)\n",
    "\n",
    "# Probabilities\n",
    "normalise_probs_in_place(dfs, inputs, labels)\n",
    "probabilities = derive_probabilities(dfs, inputs)\n",
    "\n",
    "# Ground truth\n",
    "mapping = {label: i for i, label in enumerate(labels)}\n",
    "ground_truth = prepare_ground_truth(dfs[0], inputs, mapping)\n",
    "\n",
    "np_probs = [p[labels].to_numpy() for p in probabilities]\n",
    "np_ground_truth = ground_truth[labels].to_numpy()\n",
    "print(np_probs[0].shape)\n",
    "print(np_ground_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "low_threshold = np.full(2, 0.75, dtype=np.double)\n",
    "up_threshold = np.full(2, 0.25, dtype=np.double)\n",
    "\n",
    "all_matrices, thresholded = get_objective(np_probs, np_ground_truth, low_threshold, up_threshold)\n",
    "all_matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true match, false match, missed match, and rejects,\n",
    "summarise_results(all_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  \"cost\": {\n",
    "    \"matrix\" : [[50,500, 50],[5000,50,50], [0,0,0]],\n",
    "    \"portion_size\" : 1000,\n",
    "    \"estimate_size\":10000    # Estimate the cost for classifying N # of items\n",
    "  }\n",
    "}\n",
    "\n",
    "def calculate_cost(predicted_matrix, cost_weights, portion_size, estimate_size):\n",
    "    cost_per_prediction = np.array(cost_weights) / portion_size\n",
    "    scaled_matrix = np.round(predicted_matrix.astype(np.double) / predicted_matrix.sum() * estimate_size)\n",
    "    return scaled_matrix * cost_per_prediction\n",
    "\n",
    "# Usage guide\n",
    "calculate_cost(all_matrices[0], params[\"cost\"][\"matrix\"], params[\"cost\"][\"portion_size\"], params[\"cost\"][\"estimate_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_costs(all_matrices, cost_weights, portion_size, estimate_size):\n",
    "    cost_results = np.zeros((len(all_matrices), 3, 3))\n",
    "    for i, a in enumerate(all_matrices):\n",
    "        cost_results[i] = calculate_cost(a, cost_weights, portion_size, estimate_size)\n",
    "    return cost_results\n",
    "\n",
    "# Usage guide\n",
    "all_costs = calculate_all_costs(all_matrices, params[\"cost\"][\"matrix\"], params[\"cost\"][\"portion_size\"], params[\"cost\"][\"estimate_size\"])\n",
    "all_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_results(all_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRepair(Repair):\n",
    "\n",
    "    def _do(self, problem, pop, **kwargs):\n",
    "        for i in range(len(pop)):\n",
    "            x = pop[i].X\n",
    "            pop[i].X = np.around(x, decimals=2)\n",
    "        return pop\n",
    "\n",
    "class FindThresholds(Problem):\n",
    "    def __init__(self, all_probabilities, ground_truth, **kwargs):\n",
    "        self.all_probabilities = all_probabilities     \n",
    "        self.ground_truth = ground_truth\n",
    "        n_var = all_probabilities[0].shape[1]\n",
    "        super().__init__(n_var=n_var, \n",
    "                         n_constr=0, \n",
    "                         n_obj=4, \n",
    "                         xl=anp.zeros((n_var,), dtype=anp.int), \n",
    "                         xu=anp.ones((n_var,), dtype=anp.int),  \n",
    "                         type_var=anp.int, \n",
    "                         elementwise_evaluation=True)\n",
    "    \n",
    "    def _evaluate(self, X, out, *args, **kwargs):\n",
    "        f0 = []\n",
    "        f1 = []\n",
    "        f2 = []\n",
    "        f3 = []\n",
    "        thresholds = X \n",
    "        all_matrices, thresholded = get_objective(self.all_probabilities, self.ground_truth, thresholds[0], thresholds[1])\n",
    "        \n",
    "      \n",
    "        true_matches, false_matches, missed_matches, rejects = summarise_results(all_matrices)\n",
    "        \n",
    "        f0.append(true_matches * -1)   # True matches need to be maximised, pymoo only minimises\n",
    "        f1.append(false_matches)\n",
    "        f2.append(missed_matches)\n",
    "        f3.append(rejects)\n",
    "            \n",
    "        out[\"F\"] = anp.column_stack([f0, f1, f2, f3]).astype(anp.double)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaDistribution(Sampling):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _do(self, problem, n_samples, **kwargs):        \n",
    "        return stats.beta(a=4, b=2).rvs(size=(n_samples, problem.n_var))\n",
    "    \n",
    "algorithm = NSGA2(\n",
    "    pop_size=40,\n",
    "    n_offsprings=10,\n",
    "    repair=MyRepair(),\n",
    "    sampling= BetaDistribution(), #get_sampling(\"int_lhs\"),\n",
    "    crossover=get_crossover(\"real_sbx\", prob=0.9, eta=15),\n",
    "    mutation=get_mutation(\"real_pm\", eta=20),\n",
    ")    \n",
    "\n",
    "problem = FindThresholds(np_probs, np_ground_truth, parallelizable=(\"threads\", 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "results = None\n",
    "\n",
    "def value():\n",
    "    global results\n",
    "    results = minimize(problem,\n",
    "               algorithm,\n",
    "               (\"n_gen\", 10),\n",
    "               save_history=True,\n",
    "               verbose=True)\n",
    "timeit.timeit(value,number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = results.F\n",
    "weights = np.array([0.5,0.25,0.25,0])\n",
    "I = get_decomposition(\"weighted-sum\").do(F, weights).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defails of Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values from the final population \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4454c00b37bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Values from the final population \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Values from the final population \")\n",
    "results.pop.get(\"X\"), results.pop.get(\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-db5ded3b33d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimisation Results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Lower Threshold:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Upper Threshold:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "lower, upper = results.X[I]\n",
    "print(\"Optimisation Results\")\n",
    "print(\"Best Lower Threshold:\", lower)\n",
    "print(\"Best Upper Threshold:\", upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lower' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f265a4f40ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlow_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mup_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# True match, false match, missed match, rejects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_matrices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ground_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrue_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissed_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarise_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_matrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lower' is not defined"
     ]
    }
   ],
   "source": [
    "low_threshold = np.full(2, lower, dtype=np.double)\n",
    "up_threshold = np.full(2, upper, dtype=np.double)\n",
    "# True match, false match, missed match, rejects\n",
    "all_matrices, thresholded = get_objective(np_probs, np_ground_truth, low_threshold, up_threshold)\n",
    "true_match, false_match, missed_match, rejects = summarise_results(all_matrices)\n",
    "print(\"True Match:\",true_match)\n",
    "print(\"False Match:\",false_match)\n",
    "print(\"Missed Match:\",missed_match)\n",
    "print(\"Rejects:\",rejects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
